{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine,text\n",
    "import psycopg2\n",
    "import os\n",
    "import io\n",
    "import time \n",
    "import numpy as np\n",
    "from psycopg2 import sql\n",
    "import timeit\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_postgres_engine(user, password, host, port, db_name):\n",
    "    \"\"\"Create a SQLAlchemy engine for PostgreSQL.\"\"\"\n",
    "    connection_string = f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{db_name}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_postgres_engine(config.DB_USER,config.DB_PASSWORD,config.DB_HOST,config.DB_PORT,config.DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_file(filename):\n",
    "    # Open and read the file\n",
    "    with open(filename, 'r') as file:\n",
    "        sql_script = file.read()\n",
    "    \n",
    "    # Begin a connection\n",
    "    with engine.connect() as connection:\n",
    "        # Start a transaction\n",
    "        with connection.begin():\n",
    "            # Split script into individual statements\n",
    "            statements = sql_script.split(';')\n",
    "            \n",
    "            # Execute each statement\n",
    "            for statement in statements:\n",
    "                # Remove whitespace\n",
    "                clean_statement = statement.strip()\n",
    "                \n",
    "                # Skip empty statements\n",
    "                if clean_statement:\n",
    "                    try:\n",
    "                        # Execute each statement\n",
    "                        connection.execute(text(clean_statement))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error executing statement: {clean_statement}\")\n",
    "                        print(f\"Error details: {e}\")\n",
    "                        raise\n",
    "        \n",
    "        print(f\"SQL file {filename} executed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table_into_db(file_path, table_name, conn_params):\n",
    "\n",
    "    metrics = {\n",
    "        'file_name': file_path.split(\"/\")[-1],\n",
    "        'insertion_time_ms': 0,\n",
    "        'wall_time_ms': 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Establish connection\n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "        conn.set_session(autocommit=False)\n",
    "        \n",
    "        try:\n",
    "            wall_start_time = time.time()\n",
    "\n",
    "            with conn.cursor() as cur:\n",
    "                # Start timing\n",
    "               \n",
    "                # Open the CSV file and copy\n",
    "                with open(file_path, 'r') as f:\n",
    "                    insertion_start = timeit.default_timer()\n",
    "                    cur.copy_expert(\n",
    "                        sql.SQL('COPY {} FROM STDIN WITH (FORMAT CSV, HEADER TRUE)').format(\n",
    "                            sql.Identifier(table_name)\n",
    "                        ), \n",
    "                        f\n",
    "                    )\n",
    "                    insertion_end = timeit.default_timer()\n",
    "                    metrics['insertion_time_ms'] = (insertion_end - insertion_start) * 1000\n",
    "                   \n",
    "                # Commit the transaction\n",
    "                conn.commit()\n",
    "                \n",
    "                # Calculate wall time\n",
    "                wall_end_time = time.time()\n",
    "                metrics['wall_time_ms'] = (wall_end_time - wall_start_time) * 1000\n",
    "\n",
    "                # Print metrics\n",
    "                print(f\"Import Metrics for {file_path}:\")\n",
    "                print(f\"Insertion Time: {metrics['insertion_time_ms']:.2f} ms\")\n",
    "                print(f\"Wall Time: {metrics['wall_time_ms']:.2f} ms\")\n",
    "\n",
    "        \n",
    "        \n",
    "        except Exception as inner_e:\n",
    "            # Rollback if any error occurs\n",
    "            conn.rollback()\n",
    "            print(f\"Error importing {file_path}: {inner_e}\")\n",
    "        \n",
    "        finally:\n",
    "            # Ensure connection is closed\n",
    "            conn.close()\n",
    "            return metrics \n",
    "    \n",
    "    except psycopg2.Error as conn_e:\n",
    "        print(f\"Database connection error: {conn_e}\")\n",
    "        return metrics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_params = {\n",
    "    'dbname': config.DB_NAME,\n",
    "    'user': config.DB_USER,\n",
    "    'password': config.DB_PASSWORD,\n",
    "    'host': config.DB_HOST,\n",
    "    'port': config.DB_PORT\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = {\n",
    "    'ACC':'accelerometer_data',\n",
    "    'BVP':'blood_volume_pulse',\n",
    "    'Dexcom':'interstitial_glucose',\n",
    "    'EDA':'electrodermal_activity',\n",
    "    'HR':'heart_rate_data',\n",
    "    'IBI':'ibi_data',\n",
    "    'TEMP':'temperature_data'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mention scale factor\n",
    "scale_factor = config.SCALE_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_to_places_string(number):\n",
    "    \n",
    "    # Ensure the input is a valid integer within range\n",
    "    if not isinstance(number, int) or not (0 <= number <= 999):\n",
    "        raise ValueError(\"Input must be an integer between 0 and 999.\")\n",
    "\n",
    "    # Extract hundreds, tens, and ones\n",
    "    hundreds = number // 100\n",
    "    tens = (number // 10) % 10\n",
    "    ones = number % 10\n",
    "\n",
    "    # Format into the desired string\n",
    "    result = f\"{hundreds}{tens}{ones}\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_use = [integer_to_places_string(i) for i in range(1,scale_factor+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_files = ['ACC','BVP','Dexcom','EDA','HR','IBI','TEMP']  ## if want to ignore a table remove it from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL file sql_scripts/create_schema.sql executed successfully!\n",
      "Import Metrics for ../new_data/Demographics.csv:\n",
      "Insertion Time: 3.64 ms\n",
      "Wall Time: 4.29 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file_name': 'Demographics.csv',\n",
       " 'insertion_time_ms': 3.64137499127537,\n",
       " 'wall_time_ms': 4.293203353881836}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create Schema\n",
    "run_sql_file(os.path.join(config.SQL_SCRIPTS_PATH,'create_schema.sql'))\n",
    "\n",
    "## Load Demographics Data not to be included in data insertion timings - one time load\n",
    "demographic_path = os.path.join(config.TRANSFORM_DATA_PATH,'Demographics.csv') \n",
    "load_table_into_db(demographic_path,'demographics',conn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Metrics for ../new_data/001/ACC_001.csv:\n",
      "Insertion Time: 86838.65 ms\n",
      "Wall Time: 86842.80 ms\n",
      "Import Metrics for ../new_data/001/BVP_001.csv:\n",
      "Insertion Time: 183124.59 ms\n",
      "Wall Time: 183131.22 ms\n",
      "Import Metrics for ../new_data/001/Dexcom_001.csv:\n",
      "Insertion Time: 70.96 ms\n",
      "Wall Time: 72.92 ms\n",
      "Import Metrics for ../new_data/001/EDA_001.csv:\n",
      "Insertion Time: 11015.36 ms\n",
      "Wall Time: 11016.98 ms\n",
      "Import Metrics for ../new_data/001/HR_001.csv:\n",
      "Insertion Time: 2388.73 ms\n",
      "Wall Time: 2389.91 ms\n",
      "Import Metrics for ../new_data/001/IBI_001.csv:\n",
      "Insertion Time: 1097.70 ms\n",
      "Wall Time: 1099.44 ms\n",
      "Import Metrics for ../new_data/001/TEMP_001.csv:\n",
      "Insertion Time: 10511.01 ms\n",
      "Wall Time: 10511.99 ms\n",
      "Import Metrics for ../new_data/002/ACC_002.csv:\n",
      "Insertion Time: 96011.70 ms\n",
      "Wall Time: 96014.50 ms\n",
      "Import Metrics for ../new_data/002/BVP_002.csv:\n",
      "Insertion Time: 191269.32 ms\n",
      "Wall Time: 191274.73 ms\n",
      "Import Metrics for ../new_data/002/Dexcom_002.csv:\n",
      "Insertion Time: 96.88 ms\n",
      "Wall Time: 100.25 ms\n",
      "Import Metrics for ../new_data/002/EDA_002.csv:\n",
      "Insertion Time: 11291.98 ms\n",
      "Wall Time: 11294.36 ms\n",
      "Import Metrics for ../new_data/002/HR_002.csv:\n",
      "Insertion Time: 2566.50 ms\n",
      "Wall Time: 2568.15 ms\n",
      "Import Metrics for ../new_data/002/IBI_002.csv:\n",
      "Insertion Time: 1987.92 ms\n",
      "Wall Time: 1990.49 ms\n",
      "Import Metrics for ../new_data/002/TEMP_002.csv:\n",
      "Insertion Time: 11077.80 ms\n",
      "Wall Time: 11079.80 ms\n"
     ]
    }
   ],
   "source": [
    "list_of_metrics = []\n",
    "for i in range(0,scale_factor):\n",
    "    folder_path = os.path.join(config.TRANSFORM_DATA_PATH,folder_to_use[i])\n",
    "    \n",
    "    for file in accepted_files:\n",
    "         \n",
    "        file_path = os.path.join(folder_path,f'{file}_{folder_to_use[i]}.csv')\n",
    "        metrics = load_table_into_db(file_path,table_names[file],conn_params)\n",
    "\n",
    "        list_of_metrics.append(metrics)\n",
    "\n",
    "\n",
    "report_df = pd.DataFrame(list_of_metrics)\n",
    "total_df =pd.DataFrame(report_df.select_dtypes(include=['float','int']).sum()).T \n",
    "total_df.insert(0,'file_name',['Total'])\n",
    "report_df = pd.concat([report_df,total_df],axis=0).reset_index(drop=True)\n",
    "report_df.to_csv(os.path.join(config.RESULTS_PATH,f\"insertion_stats_scale_{scale_factor}.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>insertion_time_ms</th>\n",
       "      <th>wall_time_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACC_001.csv</td>\n",
       "      <td>86838.645708</td>\n",
       "      <td>86842.803955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BVP_001.csv</td>\n",
       "      <td>183124.594791</td>\n",
       "      <td>183131.217957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dexcom_001.csv</td>\n",
       "      <td>70.959125</td>\n",
       "      <td>72.922230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDA_001.csv</td>\n",
       "      <td>11015.364792</td>\n",
       "      <td>11016.978025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HR_001.csv</td>\n",
       "      <td>2388.732958</td>\n",
       "      <td>2389.911175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IBI_001.csv</td>\n",
       "      <td>1097.700250</td>\n",
       "      <td>1099.442959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TEMP_001.csv</td>\n",
       "      <td>10511.009958</td>\n",
       "      <td>10511.991024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ACC_002.csv</td>\n",
       "      <td>96011.704417</td>\n",
       "      <td>96014.501095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BVP_002.csv</td>\n",
       "      <td>191269.318959</td>\n",
       "      <td>191274.731159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dexcom_002.csv</td>\n",
       "      <td>96.881417</td>\n",
       "      <td>100.254297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EDA_002.csv</td>\n",
       "      <td>11291.975583</td>\n",
       "      <td>11294.357061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HR_002.csv</td>\n",
       "      <td>2566.499458</td>\n",
       "      <td>2568.146229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>IBI_002.csv</td>\n",
       "      <td>1987.922250</td>\n",
       "      <td>1990.485191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TEMP_002.csv</td>\n",
       "      <td>11077.800208</td>\n",
       "      <td>11079.800844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Total</td>\n",
       "      <td>609349.109874</td>\n",
       "      <td>609387.543201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_name  insertion_time_ms   wall_time_ms\n",
       "0      ACC_001.csv       86838.645708   86842.803955\n",
       "1      BVP_001.csv      183124.594791  183131.217957\n",
       "2   Dexcom_001.csv          70.959125      72.922230\n",
       "3      EDA_001.csv       11015.364792   11016.978025\n",
       "4       HR_001.csv        2388.732958    2389.911175\n",
       "5      IBI_001.csv        1097.700250    1099.442959\n",
       "6     TEMP_001.csv       10511.009958   10511.991024\n",
       "7      ACC_002.csv       96011.704417   96014.501095\n",
       "8      BVP_002.csv      191269.318959  191274.731159\n",
       "9   Dexcom_002.csv          96.881417     100.254297\n",
       "10     EDA_002.csv       11291.975583   11294.357061\n",
       "11      HR_002.csv        2566.499458    2568.146229\n",
       "12     IBI_002.csv        1987.922250    1990.485191\n",
       "13    TEMP_002.csv       11077.800208   11079.800844\n",
       "14           Total      609349.109874  609387.543201"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## compress the data \n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows_inserted(conn_params, table_name):\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Get row count before compression\n",
    "            cur.execute(sql.SQL('SELECT COUNT(*) FROM {}').format(sql.Identifier(table_name)))\n",
    "            row_count = cur.fetchone()[0]\n",
    "            \n",
    "            # Fetch all chunks\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT chunk \n",
    "                FROM show_chunks(%s) AS chunk\n",
    "            \"\"\", (table_name,))\n",
    "            chunks = cur.fetchall()\n",
    "            \n",
    "            # Compress each chunk\n",
    "            compressed_count = 0\n",
    "            for (chunk,) in chunks:\n",
    "                try:\n",
    "                    cur.execute(\"SELECT compress_chunk(%s)\", (chunk,))\n",
    "                    compressed_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not compress chunk {chunk}: {e}\")\n",
    "            \n",
    "            conn.commit()\n",
    "            \n",
    "            return row_count\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compression process: {e}\")\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_info = {}\n",
    "for name in table_names.values():\n",
    "\n",
    "    row_info[name] = get_rows_inserted(conn_params,name)\n",
    "row_df = pd.DataFrame(row_info.items(),columns=['table_name','number_of_rows_inserted'])\n",
    "row_df.to_csv(os.path.join(config.RESULTS_PATH,f\"insertion_stats_num_rows_scale_{scale_factor}.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_name</th>\n",
       "      <th>number_of_rows_inserted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accelerometer_data</td>\n",
       "      <td>40448658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blood_volume_pulse</td>\n",
       "      <td>80897311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interstitial_glucose</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>electrodermal_activity</td>\n",
       "      <td>5056068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heart_rate_data</td>\n",
       "      <td>1263860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ibi_data</td>\n",
       "      <td>740374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>temperature_data</td>\n",
       "      <td>5056032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               table_name  number_of_rows_inserted\n",
       "0      accelerometer_data                 40448658\n",
       "1      blood_volume_pulse                 80897311\n",
       "2    interstitial_glucose                     4680\n",
       "3  electrodermal_activity                  5056068\n",
       "4         heart_rate_data                  1263860\n",
       "5                ibi_data                   740374\n",
       "6        temperature_data                  5056032"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypertable_sizes(conn_params):\n",
    "    query_1 = \"\"\"\n",
    "    SELECT\n",
    "        hypertable_name AS table_name,\n",
    "        ROUND(hypertable_size(hypertable_schema || '.' || hypertable_name)/(1024.0*1024), 4) AS total_size_mb\n",
    "        FROM\n",
    "        timescaledb_information.hypertables\n",
    "        ORDER BY\n",
    "        total_size_mb DESC;\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    df_1 = pd.read_sql_query(query_1, conn)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/51/y2_6x05j1hjgtnzwshx1dgt80000gn/T/ipykernel_66338/3989414253.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_1 = pd.read_sql_query(query_1, conn)\n"
     ]
    }
   ],
   "source": [
    "ind_size_df = get_hypertable_sizes(conn_params)\n",
    "total_df = pd.DataFrame(ind_size_df.select_dtypes(include=['float']).sum()).T\n",
    "total_df.insert(0,'table_name',['Total'])\n",
    "ind_size_df = pd.concat([ind_size_df,total_df],axis=0)\n",
    "ind_size_df.to_csv(os.path.join(config.RESULTS_PATH,f\"compression_stats_size_scale_{scale_factor}.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_query(sql_file_path, params):\n",
    "    # Read the SQL template\n",
    "    with open(sql_file_path, 'r') as file:\n",
    "        template_content = file.read()\n",
    "    \n",
    "    # Render the template with parameters\n",
    "    template = Template(template_content)\n",
    "    query = template.render(params)\n",
    "    \n",
    "    return query\n",
    "\n",
    "def execute_sql_file(conn_params, sql_file_path, params=None):\n",
    "   \n",
    "    try:\n",
    "        query = render_query(sql_file_path,params)        \n",
    "        # Establish database connection\n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "\n",
    "        with conn.cursor() as cur:\n",
    "                # Start timing\n",
    "                execution_start = timeit.default_timer()\n",
    "                cur.execute(sql.SQL(query))\n",
    "                execution_end = timeit.default_timer()\n",
    "        # Close the connection\n",
    "\n",
    "        execution_time_taken = (execution_end-execution_start)*1000\n",
    "\n",
    "        print(\"Time of Execution:\",execution_time_taken)\n",
    "        conn.close()\n",
    "        \n",
    "        return execution_time_taken\n",
    "    \n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(f\"Error executing SQL file: {error}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "         conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_queries = 9 \n",
    "number_of_times_to_run = config.NUMBER_TIMES_TO_RUN_QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_participants = [i for i in range(1,scale_factor+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of Execution: 4105.607250006869\n",
      "Time of Execution: 15.90504200430587\n",
      "Time of Execution: 39780.807917006314\n",
      "Time of Execution: 37134.109290956985\n",
      "Time of Execution: 49.77612500078976\n",
      "Time of Execution: 1905.7723329751752\n",
      "Time of Execution: 5270.645458018407\n",
      "Time of Execution: 250.1498750061728\n",
      "Time of Execution: 3493.4407089604065\n",
      "Time of Execution: 4044.8543329839595\n",
      "Time of Execution: 21.111958019901067\n",
      "Time of Execution: 36880.26004197309\n",
      "Time of Execution: 37431.189958995674\n",
      "Time of Execution: 32.44400001130998\n",
      "Time of Execution: 1863.9611250255257\n",
      "Time of Execution: 4925.843207980506\n",
      "Time of Execution: 198.46599997254089\n",
      "Time of Execution: 3389.8919590283185\n",
      "Time of Execution: 3858.947917004116\n",
      "Time of Execution: 10.925583948846906\n",
      "Time of Execution: 31333.20549997734\n",
      "Time of Execution: 35524.42129096016\n",
      "Time of Execution: 27.366750000510365\n",
      "Time of Execution: 1810.4194999905303\n",
      "Time of Execution: 4972.902458976023\n",
      "Time of Execution: 188.14337503863499\n",
      "Time of Execution: 3430.453542037867\n"
     ]
    }
   ],
   "source": [
    "execution_summary = {}\n",
    "\n",
    "for _ in range(number_of_times_to_run):\n",
    "\n",
    "    for i in range(number_of_queries): \n",
    "\n",
    "        execution_time = execute_sql_file(conn_params,os.path.join(config.SQL_SCRIPTS_PATH,f\"query_{i}.sql\"),{'list_of_participants':tuple(list_of_participants)})\n",
    "\n",
    "        if i not in execution_summary.keys():\n",
    "            execution_summary[i] = [execution_time]\n",
    "        else:\n",
    "            execution_summary[i].append(execution_time)\n",
    "\n",
    "query_df = pd.DataFrame({\n",
    "    'query_number': execution_summary.keys(),\n",
    "    'execution_times': execution_summary.values()\n",
    "})\n",
    "runs_df = pd.DataFrame(execution_summary).T\n",
    "total_run_time = runs_df.sum(axis=0).tolist()\n",
    "query_df = pd.concat([query_df,pd.DataFrame({'query_number':['total'],'execution_times':[total_run_time]})]).reset_index(drop=True)\n",
    "query_df['min_time'] = query_df['execution_times'].apply(min)\n",
    "query_df['median_time'] = query_df['execution_times'].apply(np.median)\n",
    "query_df['mean_time'] = query_df['execution_times'].apply(np.mean)\n",
    "query_df['std_dev'] = query_df['execution_times'].apply(np.std)\n",
    "query_df['max_time'] = query_df['execution_times'].apply(max)\n",
    "query_df.to_csv(os.path.join(config.RESULTS_PATH,f\"stats_query_run_time_scale_{scale_factor}.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
