{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine,text\n",
    "import psycopg2\n",
    "import os\n",
    "import io\n",
    "import time \n",
    "from psycopg2 import sql\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_postgres_engine(user, password, host, port, db_name):\n",
    "    \"\"\"Create a SQLAlchemy engine for PostgreSQL.\"\"\"\n",
    "    connection_string = f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{db_name}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_postgres_engine(config.DB_USER,config.DB_PASSWORD,config.DB_HOST,config.DB_PORT,config.DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_file(filename):\n",
    "    # Open and read the file\n",
    "    with open(filename, 'r') as file:\n",
    "        sql_script = file.read()\n",
    "    \n",
    "    # Begin a connection\n",
    "    with engine.connect() as connection:\n",
    "        # Start a transaction\n",
    "        with connection.begin():\n",
    "            # Split script into individual statements\n",
    "            statements = sql_script.split(';')\n",
    "            \n",
    "            # Execute each statement\n",
    "            for statement in statements:\n",
    "                # Remove whitespace\n",
    "                clean_statement = statement.strip()\n",
    "                \n",
    "                # Skip empty statements\n",
    "                if clean_statement:\n",
    "                    try:\n",
    "                        # Execute each statement\n",
    "                        connection.execute(text(clean_statement))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error executing statement: {clean_statement}\")\n",
    "                        print(f\"Error details: {e}\")\n",
    "                        raise\n",
    "        \n",
    "        print(f\"SQL file {filename} executed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table_into_db(file_path, table_name, conn_params):\n",
    "\n",
    "    metrics = {\n",
    "        'file_name': file_path.split(\"/\")[-1],\n",
    "        'insertion_time_ms': 0,\n",
    "        'wall_time_ms': 0,\n",
    "        'rows_inserted': 0,\n",
    "        'size_in_mb': 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Establish connection\n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "        conn.set_session(autocommit=False)\n",
    "        \n",
    "        try:\n",
    "            wall_start_time = time.time()\n",
    "\n",
    "            with conn.cursor() as cur:\n",
    "                # Start timing\n",
    "               \n",
    "                # Open the CSV file and copy\n",
    "                with open(file_path, 'r') as f:\n",
    "                    insertion_start = timeit.default_timer()\n",
    "                    cur.copy_expert(\n",
    "                        sql.SQL('COPY {} FROM STDIN WITH (FORMAT CSV, HEADER TRUE)').format(\n",
    "                            sql.Identifier(table_name)\n",
    "                        ), \n",
    "                        f\n",
    "                    )\n",
    "                    insertion_end = timeit.default_timer()\n",
    "                    metrics['insertion_time_ms'] = (insertion_end - insertion_start) * 1000\n",
    "                   \n",
    "                # Commit the transaction\n",
    "                conn.commit()\n",
    "                \n",
    "                # Calculate wall time\n",
    "                wall_end_time = time.time()\n",
    "                metrics['wall_time_ms'] = (wall_end_time - wall_start_time) * 1000\n",
    "\n",
    "                cur.execute(sql.SQL('SELECT COUNT(*) FROM {}').format(sql.Identifier(table_name)))\n",
    "                metrics['rows_inserted'] = cur.fetchone()[0]\n",
    "                \n",
    "                if table_name !='demographics':\n",
    "                    cur.execute(sql.SQL(\"\"\"\n",
    "                    SELECT \n",
    "                        pg_size_pretty(hypertable_size('{}')) as total_size,\n",
    "                        hypertable_size('{}') as total_size_bytes\n",
    "                    \"\"\").format(\n",
    "                        sql.Identifier(table_name),\n",
    "                        sql.Identifier(table_name)\n",
    "                    ))\n",
    "                \n",
    "                    metrics['size_in_mb'] = round(cur.fetchone()[1] / (1024 * 1024), 4)\n",
    "                \n",
    "                # Print metrics\n",
    "                print(f\"Import Metrics for {file_path}:\")\n",
    "                print(f\"Insertion Time: {metrics['insertion_time_ms']:.2f} ms\")\n",
    "                print(f\"Wall Time: {metrics['wall_time_ms']:.2f} ms\")\n",
    "                print(f\"Rows Inserted: {metrics['rows_inserted']}\")\n",
    "                print(f\"Total Size in MB:{metrics['size_in_mb']}\")\n",
    "        \n",
    "        \n",
    "        except Exception as inner_e:\n",
    "            # Rollback if any error occurs\n",
    "            conn.rollback()\n",
    "            print(f\"Error importing {file_path}: {inner_e}\")\n",
    "        \n",
    "        finally:\n",
    "            # Ensure connection is closed\n",
    "            conn.close()\n",
    "            return metrics \n",
    "    \n",
    "    except psycopg2.Error as conn_e:\n",
    "        print(f\"Database connection error: {conn_e}\")\n",
    "        return metrics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_params = {\n",
    "    'dbname': config.DB_NAME,\n",
    "    'user': config.DB_USER,\n",
    "    'password': config.DB_PASSWORD,\n",
    "    'host': config.DB_HOST,\n",
    "    'port': config.DB_PORT\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = {\n",
    "    'ACC':'accelerometer_data',\n",
    "    'BVP':'blood_volume_pulse',\n",
    "    'Dexcom':'interstitial_glucose',\n",
    "    'EDA':'electrodermal_activity',\n",
    "    'HR':'heart_rate_data',\n",
    "    'IBI':'ibi_data',\n",
    "    'TEMP':'temperature_data'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mention scale factor\n",
    "scale_factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_to_places_string(number):\n",
    "    \n",
    "    # Ensure the input is a valid integer within range\n",
    "    if not isinstance(number, int) or not (0 <= number <= 999):\n",
    "        raise ValueError(\"Input must be an integer between 0 and 999.\")\n",
    "\n",
    "    # Extract hundreds, tens, and ones\n",
    "    hundreds = number // 100\n",
    "    tens = (number // 10) % 10\n",
    "    ones = number % 10\n",
    "\n",
    "    # Format into the desired string\n",
    "    result = f\"{hundreds}{tens}{ones}\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_use = [integer_to_places_string(i) for i in range(1,scale_factor+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_files = ['ACC','BVP','Dexcom','EDA','HR','IBI','TEMP']  ## if want to ignore a table remove it from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL file sql_scripts/create_schema.sql executed successfully!\n",
      "Import Metrics for ../new_data/Demographics.csv:\n",
      "Insertion Time: 5.42 ms\n",
      "Wall Time: 6.67 ms\n",
      "Rows Inserted: 16\n",
      "Total Size in MB:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file_name': 'Demographics.csv',\n",
       " 'insertion_time_ms': 5.415791005361825,\n",
       " 'wall_time_ms': 6.6680908203125,\n",
       " 'rows_inserted': 16,\n",
       " 'size_in_mb': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create Schema\n",
    "run_sql_file(os.path.join(config.SQL_SCRIPTS_PATH,'create_schema.sql'))\n",
    "\n",
    "## Load Demographics Data not to be included in data insertion timings - one time load\n",
    "demographic_path = os.path.join(config.TRANSFORM_DATA_PATH,'Demographics.csv') \n",
    "load_table_into_db(demographic_path,'demographics',conn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Metrics for ../new_data/001/ACC_001.csv:\n",
      "Insertion Time: 97549.35 ms\n",
      "Wall Time: 97552.81 ms\n",
      "Rows Inserted: 20296428\n",
      "Total Size in MB:1947.6719\n",
      "Import Metrics for ../new_data/001/BVP_001.csv:\n",
      "Insertion Time: 184302.78 ms\n",
      "Wall Time: 184378.12 ms\n",
      "Rows Inserted: 40592838\n",
      "Total Size in MB:3583.0078\n",
      "Import Metrics for ../new_data/001/Dexcom_001.csv:\n",
      "Insertion Time: 38.86 ms\n",
      "Wall Time: 41.70 ms\n",
      "Rows Inserted: 2561\n",
      "Total Size in MB:0.4297\n",
      "Import Metrics for ../new_data/001/EDA_001.csv:\n",
      "Insertion Time: 12812.02 ms\n",
      "Wall Time: 12813.79 ms\n",
      "Rows Inserted: 2537046\n",
      "Total Size in MB:224.8281\n",
      "Import Metrics for ../new_data/001/HR_001.csv:\n",
      "Insertion Time: 2382.18 ms\n",
      "Wall Time: 2383.39 ms\n",
      "Rows Inserted: 634188\n",
      "Total Size in MB:39.6875\n",
      "Import Metrics for ../new_data/001/IBI_001.csv:\n",
      "Insertion Time: 1344.49 ms\n",
      "Wall Time: 1346.23 ms\n",
      "Rows Inserted: 266366\n",
      "Total Size in MB:23.7578\n",
      "Import Metrics for ../new_data/001/TEMP_001.csv:\n",
      "Insertion Time: 12373.77 ms\n",
      "Wall Time: 12376.09 ms\n",
      "Rows Inserted: 2537040\n",
      "Total Size in MB:224.25\n"
     ]
    }
   ],
   "source": [
    "list_of_metrics = []\n",
    "for i in range(0,scale_factor):\n",
    "    folder_path = os.path.join(config.TRANSFORM_DATA_PATH,folder_to_use[i])\n",
    "    \n",
    "    for file in accepted_files:\n",
    "         \n",
    "        file_path = os.path.join(folder_path,f'{file}_{folder_to_use[i]}.csv')\n",
    "        metrics = load_table_into_db(file_path,table_names[file],conn_params)\n",
    "\n",
    "        list_of_metrics.append(metrics)\n",
    "\n",
    "\n",
    "report_df = pd.DataFrame(list_of_metrics)\n",
    "total_df =pd.DataFrame(report_df.select_dtypes(include=['float','int']).sum()).T \n",
    "total_df.insert(0,'file_name',['Total'])\n",
    "report_df = pd.concat([report_df,total_df],axis=0).reset_index(drop=True)\n",
    "report_df.to_csv(os.path.join(config.RESULTS_PATH,f\"insertion_stats_scale_{scale_factor}.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>insertion_time_ms</th>\n",
       "      <th>wall_time_ms</th>\n",
       "      <th>rows_inserted</th>\n",
       "      <th>size_in_mb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACC_001.csv</td>\n",
       "      <td>97549.354500</td>\n",
       "      <td>97552.810907</td>\n",
       "      <td>20296428.0</td>\n",
       "      <td>1947.6719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BVP_001.csv</td>\n",
       "      <td>184302.778583</td>\n",
       "      <td>184378.121853</td>\n",
       "      <td>40592838.0</td>\n",
       "      <td>3583.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dexcom_001.csv</td>\n",
       "      <td>38.860542</td>\n",
       "      <td>41.699886</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>0.4297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDA_001.csv</td>\n",
       "      <td>12812.021291</td>\n",
       "      <td>12813.791275</td>\n",
       "      <td>2537046.0</td>\n",
       "      <td>224.8281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HR_001.csv</td>\n",
       "      <td>2382.179542</td>\n",
       "      <td>2383.385897</td>\n",
       "      <td>634188.0</td>\n",
       "      <td>39.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IBI_001.csv</td>\n",
       "      <td>1344.492250</td>\n",
       "      <td>1346.230745</td>\n",
       "      <td>266366.0</td>\n",
       "      <td>23.7578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TEMP_001.csv</td>\n",
       "      <td>12373.769500</td>\n",
       "      <td>12376.087189</td>\n",
       "      <td>2537040.0</td>\n",
       "      <td>224.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Total</td>\n",
       "      <td>310803.456208</td>\n",
       "      <td>310892.127752</td>\n",
       "      <td>66866467.0</td>\n",
       "      <td>6043.6328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_name  insertion_time_ms   wall_time_ms  rows_inserted  size_in_mb\n",
       "0     ACC_001.csv       97549.354500   97552.810907     20296428.0   1947.6719\n",
       "1     BVP_001.csv      184302.778583  184378.121853     40592838.0   3583.0078\n",
       "2  Dexcom_001.csv          38.860542      41.699886         2561.0      0.4297\n",
       "3     EDA_001.csv       12812.021291   12813.791275      2537046.0    224.8281\n",
       "4      HR_001.csv        2382.179542    2383.385897       634188.0     39.6875\n",
       "5     IBI_001.csv        1344.492250    1346.230745       266366.0     23.7578\n",
       "6    TEMP_001.csv       12373.769500   12376.087189      2537040.0    224.2500\n",
       "7           Total      310803.456208  310892.127752     66866467.0   6043.6328"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
