{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine,text\n",
    "import psycopg2\n",
    "import os\n",
    "import io\n",
    "import time \n",
    "import numpy as np\n",
    "from psycopg2 import sql\n",
    "import timeit\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_postgres_engine(user, password, host, port, db_name):\n",
    "    \"\"\"Create a SQLAlchemy engine for PostgreSQL.\"\"\"\n",
    "    connection_string = f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{db_name}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_postgres_engine(config.DB_USER,config.DB_PASSWORD,config.DB_HOST,config.DB_PORT,config.DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_file(filename):\n",
    "    # Open and read the file\n",
    "    with open(filename, 'r') as file:\n",
    "        sql_script = file.read()\n",
    "    \n",
    "    # Begin a connection\n",
    "    with engine.connect() as connection:\n",
    "        # Start a transaction\n",
    "        with connection.begin():\n",
    "            # Split script into individual statements\n",
    "            statements = sql_script.split(';')\n",
    "            \n",
    "            # Execute each statement\n",
    "            for statement in statements:\n",
    "                # Remove whitespace\n",
    "                clean_statement = statement.strip()\n",
    "                \n",
    "                # Skip empty statements\n",
    "                if clean_statement:\n",
    "                    try:\n",
    "                        # Execute each statement\n",
    "                        connection.execute(text(clean_statement))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error executing statement: {clean_statement}\")\n",
    "                        print(f\"Error details: {e}\")\n",
    "                        raise\n",
    "        \n",
    "        print(f\"SQL file {filename} executed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table_into_db(file_path, table_name, conn_params):\n",
    "\n",
    "    metrics = {\n",
    "        'file_name': file_path.split(\"/\")[-1],\n",
    "        'insertion_time_ms': 0,\n",
    "        'wall_time_ms': 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Establish connection\n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "        conn.set_session(autocommit=False)\n",
    "        \n",
    "        try:\n",
    "            wall_start_time = time.time()\n",
    "\n",
    "            with conn.cursor() as cur:\n",
    "                # Start timing\n",
    "               \n",
    "                # Open the CSV file and copy\n",
    "                with open(file_path, 'r') as f:\n",
    "                    insertion_start = timeit.default_timer()\n",
    "                    cur.copy_expert(\n",
    "                        sql.SQL('COPY {} FROM STDIN WITH (FORMAT CSV, HEADER TRUE)').format(\n",
    "                            sql.Identifier(table_name)\n",
    "                        ), \n",
    "                        f\n",
    "                    )\n",
    "                    insertion_end = timeit.default_timer()\n",
    "                    metrics['insertion_time_ms'] = (insertion_end - insertion_start) * 1000\n",
    "                   \n",
    "                # Commit the transaction\n",
    "                conn.commit()\n",
    "                \n",
    "                # Calculate wall time\n",
    "                wall_end_time = time.time()\n",
    "                metrics['wall_time_ms'] = (wall_end_time - wall_start_time) * 1000\n",
    "\n",
    "                # Print metrics\n",
    "                print(f\"Import Metrics for {file_path}:\")\n",
    "                print(f\"Insertion Time: {metrics['insertion_time_ms']:.2f} ms\")\n",
    "                print(f\"Wall Time: {metrics['wall_time_ms']:.2f} ms\")\n",
    "\n",
    "        \n",
    "        \n",
    "        except Exception as inner_e:\n",
    "            # Rollback if any error occurs\n",
    "            conn.rollback()\n",
    "            print(f\"Error importing {file_path}: {inner_e}\")\n",
    "        \n",
    "        finally:\n",
    "            # Ensure connection is closed\n",
    "            conn.close()\n",
    "            return metrics \n",
    "    \n",
    "    except psycopg2.Error as conn_e:\n",
    "        print(f\"Database connection error: {conn_e}\")\n",
    "        return metrics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_params = {\n",
    "    'dbname': config.DB_NAME,\n",
    "    'user': config.DB_USER,\n",
    "    'password': config.DB_PASSWORD,\n",
    "    'host': config.DB_HOST,\n",
    "    'port': config.DB_PORT\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = {\n",
    "    'ACC':'accelerometer_data',\n",
    "    'BVP':'blood_volume_pulse',\n",
    "    'Dexcom':'interstitial_glucose',\n",
    "    'EDA':'electrodermal_activity',\n",
    "    'HR':'heart_rate_data',\n",
    "    'IBI':'ibi_data',\n",
    "    'TEMP':'temperature_data'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mention scale factor\n",
    "scale_factor = config.SCALE_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_to_places_string(number):\n",
    "    \n",
    "    # Ensure the input is a valid integer within range\n",
    "    if not isinstance(number, int) or not (0 <= number <= 999):\n",
    "        raise ValueError(\"Input must be an integer between 0 and 999.\")\n",
    "\n",
    "    # Extract hundreds, tens, and ones\n",
    "    hundreds = number // 100\n",
    "    tens = (number // 10) % 10\n",
    "    ones = number % 10\n",
    "\n",
    "    # Format into the desired string\n",
    "    result = f\"{hundreds}{tens}{ones}\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_use = [integer_to_places_string(i) for i in range(1,scale_factor+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_files = ['ACC','BVP','Dexcom','EDA','HR','IBI','TEMP']  ## if want to ignore a table remove it from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL file sql_scripts/create_schema.sql executed successfully!\n",
      "Import Metrics for ../new_data/Demographics.csv:\n",
      "Insertion Time: 4.16 ms\n",
      "Wall Time: 8.75 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file_name': 'Demographics.csv',\n",
       " 'insertion_time_ms': 4.155767001066124,\n",
       " 'wall_time_ms': 8.748769760131836}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create Schema\n",
    "run_sql_file(os.path.join(config.SQL_SCRIPTS_PATH,'create_schema.sql'))\n",
    "\n",
    "## Load Demographics Data not to be included in data insertion timings - one time load\n",
    "demographic_path = os.path.join(config.TRANSFORM_DATA_PATH,'Demographics.csv') \n",
    "load_table_into_db(demographic_path,'demographics',conn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Metrics for ../new_data/001/ACC_001.csv:\n",
      "Insertion Time: 86601.98 ms\n",
      "Wall Time: 86627.15 ms\n",
      "Import Metrics for ../new_data/001/BVP_001.csv:\n",
      "Insertion Time: 165619.92 ms\n",
      "Wall Time: 165627.30 ms\n",
      "Import Metrics for ../new_data/001/Dexcom_001.csv:\n",
      "Insertion Time: 59.76 ms\n",
      "Wall Time: 62.28 ms\n",
      "Import Metrics for ../new_data/001/EDA_001.csv:\n",
      "Insertion Time: 10118.97 ms\n",
      "Wall Time: 10126.05 ms\n",
      "Import Metrics for ../new_data/001/HR_001.csv:\n",
      "Insertion Time: 2344.69 ms\n",
      "Wall Time: 2355.97 ms\n",
      "Import Metrics for ../new_data/001/IBI_001.csv:\n",
      "Insertion Time: 1204.44 ms\n",
      "Wall Time: 1209.76 ms\n",
      "Import Metrics for ../new_data/001/TEMP_001.csv:\n",
      "Insertion Time: 10859.72 ms\n",
      "Wall Time: 10865.52 ms\n",
      "Import Metrics for ../new_data/002/ACC_002.csv:\n",
      "Insertion Time: 86441.91 ms\n",
      "Wall Time: 86459.82 ms\n",
      "Import Metrics for ../new_data/002/BVP_002.csv:\n",
      "Insertion Time: 164390.67 ms\n",
      "Wall Time: 164396.16 ms\n",
      "Import Metrics for ../new_data/002/Dexcom_002.csv:\n",
      "Insertion Time: 79.66 ms\n",
      "Wall Time: 83.27 ms\n",
      "Import Metrics for ../new_data/002/EDA_002.csv:\n",
      "Insertion Time: 10244.63 ms\n",
      "Wall Time: 10251.33 ms\n",
      "Import Metrics for ../new_data/002/HR_002.csv:\n",
      "Insertion Time: 2412.92 ms\n",
      "Wall Time: 2419.07 ms\n",
      "Import Metrics for ../new_data/002/IBI_002.csv:\n",
      "Insertion Time: 1887.77 ms\n",
      "Wall Time: 1892.46 ms\n",
      "Import Metrics for ../new_data/002/TEMP_002.csv:\n",
      "Insertion Time: 10029.72 ms\n",
      "Wall Time: 10032.63 ms\n",
      "Import Metrics for ../new_data/003/ACC_003.csv:\n",
      "Insertion Time: 56116.41 ms\n",
      "Wall Time: 56120.16 ms\n",
      "Import Metrics for ../new_data/003/BVP_003.csv:\n",
      "Insertion Time: 106114.36 ms\n",
      "Wall Time: 106120.66 ms\n",
      "Import Metrics for ../new_data/003/Dexcom_003.csv:\n",
      "Insertion Time: 23.79 ms\n",
      "Wall Time: 27.86 ms\n",
      "Import Metrics for ../new_data/003/EDA_003.csv:\n",
      "Insertion Time: 6443.32 ms\n",
      "Wall Time: 6452.77 ms\n",
      "Import Metrics for ../new_data/003/HR_003.csv:\n",
      "Insertion Time: 1549.20 ms\n",
      "Wall Time: 1556.91 ms\n",
      "Import Metrics for ../new_data/003/IBI_003.csv:\n",
      "Insertion Time: 661.65 ms\n",
      "Wall Time: 665.53 ms\n",
      "Import Metrics for ../new_data/003/TEMP_003.csv:\n",
      "Insertion Time: 6728.48 ms\n",
      "Wall Time: 6733.67 ms\n",
      "Import Metrics for ../new_data/004/ACC_004.csv:\n",
      "Insertion Time: 67873.13 ms\n",
      "Wall Time: 67882.68 ms\n",
      "Import Metrics for ../new_data/004/BVP_004.csv:\n",
      "Insertion Time: 117075.86 ms\n",
      "Wall Time: 117083.85 ms\n",
      "Import Metrics for ../new_data/004/Dexcom_004.csv:\n",
      "Insertion Time: 38.23 ms\n",
      "Wall Time: 42.73 ms\n",
      "Import Metrics for ../new_data/004/EDA_004.csv:\n",
      "Insertion Time: 7422.66 ms\n",
      "Wall Time: 7429.32 ms\n",
      "Import Metrics for ../new_data/004/HR_004.csv:\n",
      "Insertion Time: 1894.88 ms\n",
      "Wall Time: 1904.33 ms\n",
      "Import Metrics for ../new_data/004/IBI_004.csv:\n",
      "Insertion Time: 1016.17 ms\n",
      "Wall Time: 1038.73 ms\n",
      "Import Metrics for ../new_data/004/TEMP_004.csv:\n",
      "Insertion Time: 7364.54 ms\n",
      "Wall Time: 7368.57 ms\n",
      "Import Metrics for ../new_data/005/ACC_005.csv:\n",
      "Insertion Time: 97911.16 ms\n",
      "Wall Time: 97931.53 ms\n",
      "Import Metrics for ../new_data/005/BVP_005.csv:\n",
      "Insertion Time: 188871.45 ms\n",
      "Wall Time: 188893.00 ms\n",
      "Import Metrics for ../new_data/005/Dexcom_005.csv:\n",
      "Insertion Time: 61.30 ms\n",
      "Wall Time: 381.66 ms\n",
      "Import Metrics for ../new_data/005/EDA_005.csv:\n",
      "Insertion Time: 11583.88 ms\n",
      "Wall Time: 11603.61 ms\n",
      "Import Metrics for ../new_data/005/HR_005.csv:\n",
      "Insertion Time: 2813.16 ms\n",
      "Wall Time: 2829.55 ms\n",
      "Import Metrics for ../new_data/005/IBI_005.csv:\n",
      "Insertion Time: 1085.35 ms\n",
      "Wall Time: 1102.74 ms\n",
      "Import Metrics for ../new_data/005/TEMP_005.csv:\n",
      "Insertion Time: 11610.51 ms\n",
      "Wall Time: 11626.05 ms\n"
     ]
    }
   ],
   "source": [
    "list_of_metrics = []\n",
    "for i in range(0,scale_factor):\n",
    "    folder_path = os.path.join(config.TRANSFORM_DATA_PATH,folder_to_use[i])\n",
    "    \n",
    "    for file in accepted_files:\n",
    "         \n",
    "        file_path = os.path.join(folder_path,f'{file}_{folder_to_use[i]}.csv')\n",
    "        metrics = load_table_into_db(file_path,table_names[file],conn_params)\n",
    "\n",
    "        list_of_metrics.append(metrics)\n",
    "\n",
    "\n",
    "report_df = pd.DataFrame(list_of_metrics)\n",
    "total_df =pd.DataFrame(report_df.select_dtypes(include=['float','int']).sum()).T \n",
    "total_df.insert(0,'file_name',['Total'])\n",
    "report_df = pd.concat([report_df,total_df],axis=0).reset_index(drop=True)\n",
    "report_df.to_csv(os.path.join(config.RESULTS_PATH,f\"insertion_stats_scale_{scale_factor}.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>insertion_time_ms</th>\n",
       "      <th>wall_time_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACC_001.csv</td>\n",
       "      <td>8.660198e+04</td>\n",
       "      <td>8.662715e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BVP_001.csv</td>\n",
       "      <td>1.656199e+05</td>\n",
       "      <td>1.656273e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dexcom_001.csv</td>\n",
       "      <td>5.975688e+01</td>\n",
       "      <td>6.228352e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDA_001.csv</td>\n",
       "      <td>1.011897e+04</td>\n",
       "      <td>1.012605e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HR_001.csv</td>\n",
       "      <td>2.344692e+03</td>\n",
       "      <td>2.355971e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IBI_001.csv</td>\n",
       "      <td>1.204438e+03</td>\n",
       "      <td>1.209763e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TEMP_001.csv</td>\n",
       "      <td>1.085972e+04</td>\n",
       "      <td>1.086552e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ACC_002.csv</td>\n",
       "      <td>8.644191e+04</td>\n",
       "      <td>8.645982e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BVP_002.csv</td>\n",
       "      <td>1.643907e+05</td>\n",
       "      <td>1.643962e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dexcom_002.csv</td>\n",
       "      <td>7.965911e+01</td>\n",
       "      <td>8.327150e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EDA_002.csv</td>\n",
       "      <td>1.024463e+04</td>\n",
       "      <td>1.025133e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HR_002.csv</td>\n",
       "      <td>2.412920e+03</td>\n",
       "      <td>2.419070e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>IBI_002.csv</td>\n",
       "      <td>1.887768e+03</td>\n",
       "      <td>1.892456e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TEMP_002.csv</td>\n",
       "      <td>1.002972e+04</td>\n",
       "      <td>1.003263e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ACC_003.csv</td>\n",
       "      <td>5.611641e+04</td>\n",
       "      <td>5.612016e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BVP_003.csv</td>\n",
       "      <td>1.061144e+05</td>\n",
       "      <td>1.061207e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Dexcom_003.csv</td>\n",
       "      <td>2.378732e+01</td>\n",
       "      <td>2.786088e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>EDA_003.csv</td>\n",
       "      <td>6.443321e+03</td>\n",
       "      <td>6.452769e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HR_003.csv</td>\n",
       "      <td>1.549198e+03</td>\n",
       "      <td>1.556912e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>IBI_003.csv</td>\n",
       "      <td>6.616468e+02</td>\n",
       "      <td>6.655345e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TEMP_003.csv</td>\n",
       "      <td>6.728476e+03</td>\n",
       "      <td>6.733675e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ACC_004.csv</td>\n",
       "      <td>6.787313e+04</td>\n",
       "      <td>6.788268e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BVP_004.csv</td>\n",
       "      <td>1.170759e+05</td>\n",
       "      <td>1.170839e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Dexcom_004.csv</td>\n",
       "      <td>3.822552e+01</td>\n",
       "      <td>4.273272e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>EDA_004.csv</td>\n",
       "      <td>7.422660e+03</td>\n",
       "      <td>7.429317e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>HR_004.csv</td>\n",
       "      <td>1.894882e+03</td>\n",
       "      <td>1.904334e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IBI_004.csv</td>\n",
       "      <td>1.016170e+03</td>\n",
       "      <td>1.038733e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TEMP_004.csv</td>\n",
       "      <td>7.364543e+03</td>\n",
       "      <td>7.368569e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ACC_005.csv</td>\n",
       "      <td>9.791116e+04</td>\n",
       "      <td>9.793153e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BVP_005.csv</td>\n",
       "      <td>1.888715e+05</td>\n",
       "      <td>1.888930e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Dexcom_005.csv</td>\n",
       "      <td>6.130191e+01</td>\n",
       "      <td>3.816564e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>EDA_005.csv</td>\n",
       "      <td>1.158388e+04</td>\n",
       "      <td>1.160361e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>HR_005.csv</td>\n",
       "      <td>2.813161e+03</td>\n",
       "      <td>2.829552e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>IBI_005.csv</td>\n",
       "      <td>1.085350e+03</td>\n",
       "      <td>1.102737e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>TEMP_005.csv</td>\n",
       "      <td>1.161051e+04</td>\n",
       "      <td>1.162605e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Total</td>\n",
       "      <td>1.246556e+06</td>\n",
       "      <td>1.247205e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_name  insertion_time_ms  wall_time_ms\n",
       "0      ACC_001.csv       8.660198e+04  8.662715e+04\n",
       "1      BVP_001.csv       1.656199e+05  1.656273e+05\n",
       "2   Dexcom_001.csv       5.975688e+01  6.228352e+01\n",
       "3      EDA_001.csv       1.011897e+04  1.012605e+04\n",
       "4       HR_001.csv       2.344692e+03  2.355971e+03\n",
       "5      IBI_001.csv       1.204438e+03  1.209763e+03\n",
       "6     TEMP_001.csv       1.085972e+04  1.086552e+04\n",
       "7      ACC_002.csv       8.644191e+04  8.645982e+04\n",
       "8      BVP_002.csv       1.643907e+05  1.643962e+05\n",
       "9   Dexcom_002.csv       7.965911e+01  8.327150e+01\n",
       "10     EDA_002.csv       1.024463e+04  1.025133e+04\n",
       "11      HR_002.csv       2.412920e+03  2.419070e+03\n",
       "12     IBI_002.csv       1.887768e+03  1.892456e+03\n",
       "13    TEMP_002.csv       1.002972e+04  1.003263e+04\n",
       "14     ACC_003.csv       5.611641e+04  5.612016e+04\n",
       "15     BVP_003.csv       1.061144e+05  1.061207e+05\n",
       "16  Dexcom_003.csv       2.378732e+01  2.786088e+01\n",
       "17     EDA_003.csv       6.443321e+03  6.452769e+03\n",
       "18      HR_003.csv       1.549198e+03  1.556912e+03\n",
       "19     IBI_003.csv       6.616468e+02  6.655345e+02\n",
       "20    TEMP_003.csv       6.728476e+03  6.733675e+03\n",
       "21     ACC_004.csv       6.787313e+04  6.788268e+04\n",
       "22     BVP_004.csv       1.170759e+05  1.170839e+05\n",
       "23  Dexcom_004.csv       3.822552e+01  4.273272e+01\n",
       "24     EDA_004.csv       7.422660e+03  7.429317e+03\n",
       "25      HR_004.csv       1.894882e+03  1.904334e+03\n",
       "26     IBI_004.csv       1.016170e+03  1.038733e+03\n",
       "27    TEMP_004.csv       7.364543e+03  7.368569e+03\n",
       "28     ACC_005.csv       9.791116e+04  9.793153e+04\n",
       "29     BVP_005.csv       1.888715e+05  1.888930e+05\n",
       "30  Dexcom_005.csv       6.130191e+01  3.816564e+02\n",
       "31     EDA_005.csv       1.158388e+04  1.160361e+04\n",
       "32      HR_005.csv       2.813161e+03  2.829552e+03\n",
       "33     IBI_005.csv       1.085350e+03  1.102737e+03\n",
       "34    TEMP_005.csv       1.161051e+04  1.162605e+04\n",
       "35           Total       1.246556e+06  1.247205e+06"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## compress the data \n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows_inserted(conn_params, table_name):\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Get row count before compression\n",
    "            cur.execute(sql.SQL('SELECT COUNT(*) FROM {}').format(sql.Identifier(table_name)))\n",
    "            row_count = cur.fetchone()[0]\n",
    "            \n",
    "            # Fetch all chunks\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT chunk \n",
    "                FROM show_chunks(%s) AS chunk\n",
    "            \"\"\", (table_name,))\n",
    "            chunks = cur.fetchall()\n",
    "            \n",
    "            # Compress each chunk\n",
    "            compressed_count = 0\n",
    "            for (chunk,) in chunks:\n",
    "                try:\n",
    "                    cur.execute(\"SELECT compress_chunk(%s)\", (chunk,))\n",
    "                    compressed_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not compress chunk {chunk}: {e}\")\n",
    "            \n",
    "            conn.commit()\n",
    "            \n",
    "            return row_count\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compression process: {e}\")\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_info = {}\n",
    "for name in table_names.values():\n",
    "\n",
    "    row_info[name] = get_rows_inserted(conn_params,name)\n",
    "row_df = pd.DataFrame(row_info.items(),columns=['table_name','number_of_rows_inserted'])\n",
    "row_df.to_csv(os.path.join(config.RESULTS_PATH,f\"insertion_stats_num_rows_scale_{scale_factor}.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_name</th>\n",
       "      <th>number_of_rows_inserted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accelerometer_data</td>\n",
       "      <td>91510446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blood_volume_pulse</td>\n",
       "      <td>183020849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interstitial_glucose</td>\n",
       "      <td>11702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>electrodermal_activity</td>\n",
       "      <td>11438736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heart_rate_data</td>\n",
       "      <td>2859141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ibi_data</td>\n",
       "      <td>1382424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>temperature_data</td>\n",
       "      <td>11438656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               table_name  number_of_rows_inserted\n",
       "0      accelerometer_data                 91510446\n",
       "1      blood_volume_pulse                183020849\n",
       "2    interstitial_glucose                    11702\n",
       "3  electrodermal_activity                 11438736\n",
       "4         heart_rate_data                  2859141\n",
       "5                ibi_data                  1382424\n",
       "6        temperature_data                 11438656"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypertable_sizes(conn_params):\n",
    "    query_1 = \"\"\"\n",
    "    SELECT\n",
    "        hypertable_name AS table_name,\n",
    "        ROUND(hypertable_size(hypertable_schema || '.' || hypertable_name)/(1024.0*1024), 4) AS total_size_mb\n",
    "        FROM\n",
    "        timescaledb_information.hypertables\n",
    "        ORDER BY\n",
    "        total_size_mb DESC;\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    df_1 = pd.read_sql_query(query_1, conn)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16164/3989414253.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_1 = pd.read_sql_query(query_1, conn)\n"
     ]
    }
   ],
   "source": [
    "ind_size_df = get_hypertable_sizes(conn_params)\n",
    "total_df = pd.DataFrame(ind_size_df.select_dtypes(include=['float']).sum()).T\n",
    "total_df.insert(0,'table_name',['Total'])\n",
    "ind_size_df = pd.concat([ind_size_df,total_df],axis=0)\n",
    "ind_size_df.to_csv(os.path.join(config.RESULTS_PATH,f\"compression_stats_size_scale_{scale_factor}.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_query(sql_file_path, params):\n",
    "    # Read the SQL template\n",
    "    with open(sql_file_path, 'r') as file:\n",
    "        template_content = file.read()\n",
    "    \n",
    "    # Render the template with parameters\n",
    "    template = Template(template_content)\n",
    "    query = template.render(params)\n",
    "    \n",
    "    return query\n",
    "\n",
    "def execute_sql_file(conn_params, sql_file_path, params=None):\n",
    "   \n",
    "    try:\n",
    "        query = render_query(sql_file_path,params)        \n",
    "        # Establish database connection\n",
    "        conn = psycopg2.connect(**conn_params)\n",
    "\n",
    "        with conn.cursor() as cur:\n",
    "                # Start timing\n",
    "                execution_start = timeit.default_timer()\n",
    "                cur.execute(sql.SQL(query))\n",
    "                execution_end = timeit.default_timer()\n",
    "        # Close the connection\n",
    "\n",
    "        execution_time_taken = (execution_end-execution_start)*1000\n",
    "\n",
    "        print(\"Time of Execution:\",execution_time_taken)\n",
    "        conn.close()\n",
    "        \n",
    "        return execution_time_taken\n",
    "    \n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(f\"Error executing SQL file: {error}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "         conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_queries = 9 \n",
    "number_of_times_to_run = config.NUMBER_TIMES_TO_RUN_QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_participants = [i for i in range(1,scale_factor+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of Execution: 8446.659896999336\n",
      "Time of Execution: 15.766841999720782\n",
      "Time of Execution: 69844.81431200038\n",
      "Time of Execution: 76109.06143600005\n",
      "Time of Execution: 18.493386000045575\n",
      "Time of Execution: 3663.4492429984675\n",
      "Time of Execution: 11289.980662000744\n",
      "Time of Execution: 462.6918400026625\n",
      "Time of Execution: 3429.5022919977782\n",
      "Time of Execution: 8639.549784002156\n",
      "Time of Execution: 14.443983000091976\n",
      "Time of Execution: 69534.11541499736\n",
      "Time of Execution: 74968.17872999964\n",
      "Time of Execution: 19.282425000710646\n",
      "Time of Execution: 3672.237688999303\n",
      "Time of Execution: 11312.797072001558\n",
      "Time of Execution: 457.3032750013226\n",
      "Time of Execution: 3365.700743997877\n",
      "Time of Execution: 8425.37604899917\n",
      "Time of Execution: 14.79256700258702\n",
      "Time of Execution: 69538.11593799765\n",
      "Time of Execution: 74150.35592999993\n",
      "Time of Execution: 18.4987419997924\n",
      "Time of Execution: 3657.7705099989544\n",
      "Time of Execution: 11428.534763999778\n",
      "Time of Execution: 447.80755400279304\n",
      "Time of Execution: 3342.328692000592\n",
      "Time of Execution: 8339.97059199828\n",
      "Time of Execution: 14.206744999682996\n",
      "Time of Execution: 65629.82719499996\n",
      "Time of Execution: 74141.94756099823\n",
      "Time of Execution: 21.502672003407497\n",
      "Time of Execution: 3646.4528900032747\n",
      "Time of Execution: 11328.735658000369\n",
      "Time of Execution: 452.54469200153835\n",
      "Time of Execution: 3441.6419730005146\n",
      "Time of Execution: 8295.458289001544\n",
      "Time of Execution: 14.472188999206992\n",
      "Time of Execution: 66912.7270320023\n",
      "Time of Execution: 74920.594231\n",
      "Time of Execution: 24.61883499927353\n",
      "Time of Execution: 3899.2951559994253\n",
      "Time of Execution: 11521.947597997496\n",
      "Time of Execution: 460.39490299881436\n",
      "Time of Execution: 3379.1967630022555\n",
      "Time of Execution: 8417.462947996682\n",
      "Time of Execution: 14.139692000753712\n",
      "Time of Execution: 65890.10553199842\n",
      "Time of Execution: 75711.7996119996\n",
      "Time of Execution: 17.513308997877175\n",
      "Time of Execution: 3693.417098002101\n",
      "Time of Execution: 11238.644552999176\n",
      "Time of Execution: 458.2044330018107\n",
      "Time of Execution: 3404.4288950026385\n",
      "Time of Execution: 8790.310406999197\n",
      "Time of Execution: 14.871886000037193\n",
      "Time of Execution: 66292.47774300165\n",
      "Time of Execution: 76209.3829789992\n",
      "Time of Execution: 17.79956999962451\n",
      "Time of Execution: 3666.2482179999643\n",
      "Time of Execution: 11411.102510999626\n",
      "Time of Execution: 460.5829009997251\n",
      "Time of Execution: 3394.5140859977982\n",
      "Time of Execution: 8473.737756001356\n",
      "Time of Execution: 14.036193002539221\n",
      "Time of Execution: 65929.75187800039\n",
      "Time of Execution: 75772.89858800214\n",
      "Time of Execution: 17.901836999953957\n",
      "Time of Execution: 3677.236642000935\n",
      "Time of Execution: 11519.225064999773\n",
      "Time of Execution: 461.0914969998703\n",
      "Time of Execution: 3509.2358000001695\n",
      "Time of Execution: 8850.916668001446\n",
      "Time of Execution: 14.091577002545819\n",
      "Time of Execution: 67080.51987300132\n",
      "Time of Execution: 75089.37628300191\n",
      "Time of Execution: 18.507110002246918\n",
      "Time of Execution: 3645.0433600002725\n",
      "Time of Execution: 11289.524561001599\n",
      "Time of Execution: 466.55276599994977\n",
      "Time of Execution: 3500.1656749991525\n",
      "Time of Execution: 8600.022203998378\n",
      "Time of Execution: 14.28517699969234\n",
      "Time of Execution: 65586.98291499968\n",
      "Time of Execution: 75765.38659499784\n",
      "Time of Execution: 26.20213100090041\n",
      "Time of Execution: 3680.5898189995787\n",
      "Time of Execution: 11637.97900300051\n",
      "Time of Execution: 459.15323900044314\n",
      "Time of Execution: 3458.6194809999142\n"
     ]
    }
   ],
   "source": [
    "execution_summary = {}\n",
    "\n",
    "for _ in range(number_of_times_to_run):\n",
    "\n",
    "    for i in range(number_of_queries): \n",
    "\n",
    "        execution_time = execute_sql_file(conn_params,os.path.join(config.SQL_SCRIPTS_PATH,f\"query_{i}.sql\"),{'list_of_participants':tuple(list_of_participants)})\n",
    "\n",
    "        if i not in execution_summary.keys():\n",
    "            execution_summary[i] = [execution_time]\n",
    "        else:\n",
    "            execution_summary[i].append(execution_time)\n",
    "\n",
    "query_df = pd.DataFrame({\n",
    "    'query_number': execution_summary.keys(),\n",
    "    'execution_times': execution_summary.values()\n",
    "})\n",
    "runs_df = pd.DataFrame(execution_summary).T\n",
    "total_run_time = runs_df.sum(axis=0).tolist()\n",
    "query_df = pd.concat([query_df,pd.DataFrame({'query_number':['total'],'execution_times':[total_run_time]})]).reset_index(drop=True)\n",
    "query_df['min_time'] = query_df['execution_times'].apply(min)\n",
    "query_df['median_time'] = query_df['execution_times'].apply(np.median)\n",
    "query_df['mean_time'] = query_df['execution_times'].apply(np.mean)\n",
    "query_df['std_dev'] = query_df['execution_times'].apply(np.std)\n",
    "query_df['max_time'] = query_df['execution_times'].apply(max)\n",
    "query_df.to_csv(os.path.join(config.RESULTS_PATH,f\"stats_query_run_time_scale_{scale_factor}.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
